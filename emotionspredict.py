# -*- coding: utf-8 -*-
"""emotionspredict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HrhiGrGJo4-dWJMxsg-cT2IBEs2ZnVFO

Import Emotions Predict Dataset
"""

import kagglehub
import os

# Download latest version
path = kagglehub.dataset_download("msambare/fer2013")

# Set extract_path to the downloaded path
extract_path = path

print("Path to dataset files:", path)

# Check folder structure
os.listdir(extract_path)

# STEP 3: Set up image paths for training and testing data
train_dir = os.path.join(extract_path, "train")
test_dir = os.path.join(extract_path, "test")

"""Dataset Preparation"""

import numpy as np
import os
from PIL import Image
import tensorflow as tf

# Get the list of emotion folders, excluding "Disgust" if present
folderList = sorted([f for f in os.listdir(train_dir) if f.lower() != 'disgust'])
print("Emotion categories:", folderList)

# Function to load grayscale 48x48 images and corresponding labels from folders
def load_images_from_folder(base_path, folderList):
    data, labels = [], []
    for idx, category in enumerate(folderList):
        category_path = os.path.join(base_path, category)
        for file in os.listdir(category_path):
            img_path = os.path.join(category_path, file)
            img = Image.open(img_path).convert('L').resize((48, 48))
            data.append(np.array(img))
            labels.append(idx)
    return np.array(data, dtype='float32'), np.array(labels)

# Load and normalize both training and test data
X_train, y_train = load_images_from_folder(train_dir, folderList)
X_test, y_test = load_images_from_folder(test_dir, folderList)

# Normalize pixel values
X_train /= 255.0
X_test /= 255.0

# Reshape data to match CNN input: (samples, height, width, channels)
X_train = X_train.reshape(-1, 48, 48, 1)
X_test = X_test.reshape(-1, 48, 48, 1)

# Convert class labels to one-hot encoded vectors
num_classes = len(folderList)
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""Model Creation"""

# Build a CNN model (VGGNet architecture)
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(48, 48, 1)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5), # Dropout to reduce overfitting
    tf.keras.layers.Dense(num_classes, activation='softmax') # Output layer
])

# Compile the model with Adam optimizer and categorical cross-entropy loss
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Display the model architecture
model.summary()

"""Training the Model"""

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Split train into train and validation sets (80% train, 20% val)
X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, shuffle=True
)

# Compute class weights to balance the influence of each class during training
y_train_labels = np.argmax(y_train_final, axis=1)
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(y_train_labels),
                                     y=y_train_labels)
class_weights = dict(enumerate(class_weights))

# Early stopping to prevent overfitting
early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)

# Create an ImageDataGenerator for augmentation on training data only
datagen = ImageDataGenerator(
    rotation_range=15,          # randomly rotate images by up to 15 degrees
    width_shift_range=0.1,      # randomly shift images horizontally by 10%
    height_shift_range=0.1,     # randomly shift images vertically by 10%
    horizontal_flip=True        # randomly flip images horizontally
)

# Train using the augmented data generator
history = model.fit(
    datagen.flow(X_train_final, y_train_final, batch_size=32),
    validation_data=(X_val, y_val), #use validation from training split
    epochs=15,
    class_weight=class_weights,
    callbacks=[early_stop]
)

"""Evaluation and Visualization"""

import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

# Plot training and validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

"""Confusion Matrix and Classification Report"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Predict class probabilities on the test set
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1) # Convert probabilities to class indices
y_true_classes = np.argmax(y_test, axis=1) # Convert one-hot to class indices

# Plot confusion matrix
cm = confusion_matrix(y_true_classes, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=folderList, yticklabels=folderList, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

# Print classification report
report = classification_report(y_true_classes, y_pred_classes, target_names=folderList)
print("Classification Report:\n")
print(report)

"""Prediction Testing"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow #For displaying images in Colab output
from google.colab import files # For file upload
import os #For file path manipulation

print ("A file dialog will appear below. Select and upload your image file.")

uploaded_image = files.upload()  #This will open a file dialog in your browser

image_file_name = None
if uploaded_image:
  for fn in uploaded_image.keys():
    image_file_name = fn  # Get the name of the uploaded image file
    print(f"User uploaded image file '{image_file_name}'")
else:
    print("No image file was uploaded. Skipping this section.")

if image_file_name:
    # --- Load the pre-trained Haar Cascade for face detection ---
    print("\nDownloading Haar Cascade classifier for face detection...")
    # This downloads the XML file containing the pre-trained features for frontal face detection.
    !wget -q https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml
    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

    if face_cascade.empty():
        print("Error: Could not load face cascade classifier. Make sure 'haarcascade_frontalface_default.xml' is available.")
        print("Skipping image processing.")
    else:
        print("Face cascade loaded successfully.")

    # --- Load the uploaded image ---
    img = cv2.imread(image_file_name)

    if img is None:
        print(f"Error: Could not read image file '{image_file_name}'. Please check the file format and integrity.")
    else:
        print(f"\n--- Starting Face Detection on '{image_file_name}' ---")

        # Convert the image to grayscale for face detection (Haar cascades work on grayscale)
        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Detect faces in the grayscale image
        # scaleFactor: Parameter specifying how much the image size is reduced at each image scale.
        #               (e.g., 1.1 means scale down by 10% each time)
        # minNeighbours: Parameter specifying how many neighbours each candidate rectangle should have to retain
        # minSize : Minimum possible object size. Objects smaller than that are ignored.
        faces = face_cascade.detectMultiScale(gray_img, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        print(f"Found {len(faces)} face(s) in the image.")

        # Create a copy of the original image to draw on
        output_img = img.copy()

        # Draw rectangles around the detected faces
        for (x, y, w, h) in faces:
          # cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)
          cv2.rectangle(output_img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green rectangle, thickness 2

        # Predict the emotion
        # Extract the face region from the grayscale image
        if len(faces) > 0:
            # For simplicity, predict emotion on the first detected face
            (x, y, w, h) = faces[0]
            face_img = gray_img[y:y+h, x:x+w]
            # Resize to 48x48 and normalize
            face_img = Image.fromarray(face_img).resize((48, 48))
            face_img_array = np.array(face_img).astype('float32') / 255.0
            face_img_array = face_img_array.reshape(1, 48, 48, 1) # Reshape for model input

            prediction = model.predict(face_img_array)
            predicted_class = np.argmax(prediction)
            predicted_emotion = folderList[predicted_class]
        else:
            predicted_emotion = "No face detected"

        # --- Display Results ---
        plt.figure(figsize=(10, 8))
        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)) # Convert BGR to RGB for matplotlib
        plt.title(f"Predicted Emotion: {predicted_emotion}")
        plt.axis('off')
        plt.show()

        # You can also save the results if needed
        base_name = os.path.splitext(image_file_name)[0]
        cv2.imwrite(f'{base_name}_faces_detected.png', output_img)
        print(f"Results saved as '{base_name}_faces_detected.png'")

else:
    print("Image face detection skipped. Please upload an image file if you wish to run this section.")